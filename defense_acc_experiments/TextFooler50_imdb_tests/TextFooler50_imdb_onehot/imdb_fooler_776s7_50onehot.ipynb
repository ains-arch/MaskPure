{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf45ef5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 19:08:39.951031: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-30 19:08:41.200646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install --upgrade tensorflow_hub\n",
    "# !pip install -U -huggingface_hub\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "import textattack\n",
    "import transformers\n",
    "import torch\n",
    "import time\n",
    "from datasets import Dataset\n",
    "import sys\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertForMaskedLM, pipeline\n",
    "from textattack.attack_recipes import (\n",
    "    TextBuggerLi2018, DeepWordBugGao2018, TextFoolerJin2019, BERTAttackLi2020\n",
    ")\n",
    "from textattack.constraints.semantics.sentence_encoders import UniversalSentenceEncoder\n",
    "from textattack.models.wrappers import ModelWrapper\n",
    "\n",
    "sys.path.append('../../')\n",
    "from eval_utils import *\n",
    "sys.path.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d2c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a device name to run on: cuda:0\n",
      "Enter the number of samples to run on (100 or 776): 776\n",
      "Specify a defense type among \"default\", \"logit\", \"maj_log\", \"one_hot\": one_hot\n",
      "enter number of candidates (recommended 12 for quicker run, 50 otherwise): 50\n",
      "Which section of the dataset would you like to load and test on? (integer between 0 and 7) : 7\n",
      "using num_voter = 11 and mask_pct = 0.3 with dataset = imdb776_into100_7...\n",
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  delete\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  50\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (min_cos_sim):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): PartOfSpeech(\n",
      "        (tagger_type):  nltk\n",
      "        (tagset):  universal\n",
      "        (allow_verb_noun_swap):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): UniversalSentenceEncoder(\n",
      "        (metric):  angular\n",
      "        (threshold):  0.7\n",
      "        (window_size):  15\n",
      "        (skip_text_shorter_than_window):  True\n",
      "        (compare_against_original):  False\n",
      "      )\n",
      "    (3): RepeatModification\n",
      "    (4): StopwordModification\n",
      "    (5): InputColumnModification(\n",
      "        (matching_column_labels):  ['premise', 'hypothesis']\n",
      "        (columns_to_ignore):  {'premise'}\n",
      "      )\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/76 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py:1081: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "2023-08-30 19:10:43.094145: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-08-30 19:10:45.187713: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype string\n",
      "\t [[{{node inputs}}]]\n",
      "[Succeeded / Failed / Skipped / Total] 1 / 9 / 0 / 10:  13%|█▎        | 10/76 [9:14:20<60:58:41, 3326.08s/it]"
     ]
    }
   ],
   "source": [
    "# set a seed, because reproducability is cool\n",
    "np.random.seed(int(hashlib.sha256('Harrison Gietz'.encode('utf-8')).hexdigest(), 16) % 2**32)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = input('enter a device name to run on: ')\n",
    "dataset_val = input('Enter the number of samples to run on (100 or 776): ')\n",
    "defense = input('Specify a defense type among \"default\", \"logit\", \"maj_log\", \"one_hot\": ')\n",
    "cand_size = int(input('enter number of candidates (recommended 12 for quicker run, 50 otherwise): '))\n",
    "\n",
    "imdb_tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-imdb\")\n",
    "imdb_model = AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-imdb\")\n",
    "imdb_model.to(device)\n",
    "imdb_pipeline = pipeline('sentiment-analysis', model=imdb_model, tokenizer=imdb_tokenizer)\n",
    "imdb_pipeline.device = next(imdb_model.parameters()).device\n",
    "\n",
    "imdb_model_directory = \"../../../../models/bert-uncased_maskedlm_imdb_july31_chk3\"\n",
    "finetuned_imdb_maskedlm = BertForMaskedLM.from_pretrained(imdb_model_directory)\n",
    "finetuned_imdb_maskedlm.to(device)\n",
    "imdb_fill_mask = pipeline(\"fill-mask\", model=finetuned_imdb_maskedlm, tokenizer=imdb_tokenizer)\n",
    "imdb_fill_mask.device = next(imdb_model.parameters()).device\n",
    "\n",
    "num_voter = 11\n",
    "mask_pct = 0.3    \n",
    "    \n",
    "attack = TextFoolerJin2019\n",
    "\n",
    "if dataset_val == '100':\n",
    "    loaded_imdb_100 = Dataset.load_from_disk('../../data/filtered_imdb_clean_100')\n",
    "    imdb_100 = textattack.datasets.Dataset(convert_to_tuples(loaded_imdb_100))\n",
    "    dataset = imdb_100\n",
    "    dataset_name = 'imdb100'\n",
    "elif dataset_val =='776':\n",
    "    # because there were problems running epxeiremtns for days on end with the larger dataset,\n",
    "    # the 776 samples were split up into 250, 250, 276 (sections 1,2,3 respectively).\n",
    "    # hence experiments are run on each section separately, with final score coming from the collective results.\n",
    "    dataset_section = input('Which section of the dataset would you like to load and test on? '\n",
    "                            '(integer between 0 and 7) : ')\n",
    "    loaded_imdb_776 = Dataset.load_from_disk(f'../../data/filtered_imdb_clean_776_into100_{dataset_section}')\n",
    "    imdb_776 = textattack.datasets.Dataset(convert_to_tuples(loaded_imdb_776))\n",
    "    dataset = imdb_776\n",
    "    dataset_name = f'imdb776_into100_{dataset_section}'\n",
    "else:\n",
    "    raise ValueError('Number of samples not supported')\n",
    "    \n",
    "if defense == \"default\":\n",
    "    imdb_wrapper = textattack.models.wrappers.HuggingFaceModelWrapper(imdb_model, imdb_tokenizer)\n",
    "elif defense == \"logit\":\n",
    "    imdb_wrapper = MaskDemaskWrapper(imdb_model, imdb_tokenizer, imdb_fill_mask, num_voter, mask_pct, 'logit')\n",
    "elif defense == 'maj_log':\n",
    "    imdb_wrapper = MaskDemaskWrapper(imdb_model, imdb_tokenizer, imdb_fill_mask, num_voter, mask_pct, 'maj_log')\n",
    "elif defense == \"one_hot\":\n",
    "    imdb_wrapper = MaskDemaskWrapper(imdb_model, imdb_tokenizer, imdb_fill_mask, num_voter, mask_pct, 'maj_one_hot')\n",
    "else:\n",
    "    raise ValueError('Not a valid defense type.')\n",
    "    \n",
    "print(f'using num_voter = {num_voter} and mask_pct = {mask_pct} with dataset = {dataset_name}...')\n",
    "\n",
    "# Parse the attack name\n",
    "attack_name = parse_attack_name(attack)\n",
    "attack = attack.build(imdb_wrapper)\n",
    "\n",
    "# change candidate size\n",
    "attack.transformation.max_candidates = cand_size\n",
    "# adjust attack threshold to match Li et al. 2023 (0.7 theshold for imdb Universal sentences encoder):\n",
    "attack.constraints[2] = UniversalSentenceEncoder(metric = 'angular', threshold = 0.7, \n",
    "                                                 window_size = 15, skip_text_shorter_than_window=True, \n",
    "                                                 compare_against_original=False)\n",
    "\n",
    "# Set up arguments for the attack\n",
    "attack_args = textattack.AttackArgs(\n",
    "    num_examples=len(dataset),\n",
    "    log_to_csv=f'{attack_name}_{dataset_name}_candsize{cand_size}_mp{mask_pct}_nv{num_voter}_{defense}_log.csv',\n",
    "    checkpoint_interval=25, \n",
    "    checkpoint_dir=\"chkpts_2\", \n",
    "    disable_stdout=True\n",
    ")\n",
    "# Perform the attack and save the results\n",
    "attacker = textattack.Attacker(attack, dataset, attack_args)\n",
    "attacker.attack_dataset()\n",
    "\n",
    "print(f'The above are results for {attack_name}_{dataset_name}_candsize{cand_size}_mp{mask_pct}_nv{num_voter}_{defense}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "13"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
