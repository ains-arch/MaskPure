{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5ca07cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 15:51:00.026410: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-08 15:51:00.944001: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install --upgrade -tensorflow_hub\n",
    "# !pip install -U -huggingface_hub\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import textattack\n",
    "import transformers\n",
    "import torch\n",
    "import time\n",
    "from datasets import Dataset\n",
    "import sys\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertForMaskedLM, pipeline\n",
    "from textattack.attack_recipes import (\n",
    "    TextBuggerLi2018, DeepWordBugGao2018, TextFoolerJin2019, BERTAttackLi2020\n",
    ")\n",
    "from textattack.constraints.semantics.sentence_encoders import UniversalSentenceEncoder\n",
    "from textattack.models.wrappers import ModelWrapper\n",
    "\n",
    "sys.path.append('../')\n",
    "from eval_utils import *\n",
    "sys.path.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f21cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a device name to run on: cuda:0\n",
      "Enter the number of samples to run on (100 or 1000): 1000\n",
      "Specify a defense type among \"default\", \"logit\", \"maj_log\", \"one_hot\": logit\n",
      "enter number of candidates (recommended 12 for quicker run, 50 otherwise): 50\n",
      "using num_voter = 11 and mask_pct = 0.3 with dataset = ag-news1000...\n",
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  delete\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  50\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (min_cos_sim):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): PartOfSpeech(\n",
      "        (tagger_type):  nltk\n",
      "        (tagset):  universal\n",
      "        (allow_verb_noun_swap):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): UniversalSentenceEncoder(\n",
      "        (metric):  angular\n",
      "        (threshold):  0.5\n",
      "        (window_size):  15\n",
      "        (skip_text_shorter_than_window):  True\n",
      "        (compare_against_original):  False\n",
      "      )\n",
      "    (3): RepeatModification\n",
      "    (4): StopwordModification\n",
      "    (5): InputColumnModification(\n",
      "        (matching_column_labels):  ['premise', 'hypothesis']\n",
      "        (columns_to_ignore):  {'premise'}\n",
      "      )\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py:1081: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "2023-08-08 15:51:37.898045: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-08-08 15:51:39.668996: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype string\n",
      "\t [[{{node inputs}}]]\n",
      "[Succeeded / Failed / Skipped / Total] 8 / 15 / 2 / 25:   2%|▎         | 25/1000 [1:56:59<76:02:53, 280.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 13 / 32 / 5 / 50:   5%|▌         | 50/1000 [3:36:56<68:41:45, 260.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 26 / 41 / 8 / 75:   8%|▊         | 75/1000 [5:12:18<64:11:50, 249.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 38 / 51 / 11 / 100:  10%|█         | 100/1000 [7:08:12<64:13:53, 256.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 51 / 62 / 12 / 125:  12%|█▎        | 125/1000 [9:04:57<63:34:40, 261.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 64 / 72 / 14 / 150:  15%|█▌        | 150/1000 [10:55:58<61:57:13, 262.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 76 / 83 / 15 / 174:  17%|█▋        | 174/1000 [12:33:38<59:37:36, 259.87s/it]"
     ]
    }
   ],
   "source": [
    "# set a seed, because reproducability is cool\n",
    "np.random.seed(int(hashlib.sha256('Harrison Gietz'.encode('utf-8')).hexdigest(), 16) % 2**32)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = input('enter a device name to run on: ')\n",
    "dataset_val = input('Enter the number of samples to run on (100 or 1000): ')\n",
    "defense = input('Specify a defense type among \"default\", \"logit\", \"maj_log\", \"one_hot\": ')\n",
    "cand_size = int(input('enter number of candidates (recommended 12 for quicker run, 50 otherwise): '))\n",
    "\n",
    "ag_tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-ag-news\")\n",
    "ag_model = AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-ag-news\")\n",
    "ag_model.to(device)\n",
    "ag_pipeline = pipeline('sentiment-analysis', model=ag_model, tokenizer=ag_tokenizer)\n",
    "ag_pipeline.device = next(ag_model.parameters()).device\n",
    "\n",
    "ag_model_directory = \"../../../models/bert-uncased_maskedlm_agnews_july31\" #first diff\n",
    "finetuned_ag_maskedlm = BertForMaskedLM.from_pretrained(ag_model_directory)\n",
    "finetuned_ag_maskedlm.to(device)\n",
    "ag_fill_mask = pipeline(\"fill-mask\", model=finetuned_ag_maskedlm, tokenizer=ag_tokenizer)\n",
    "ag_fill_mask.device = next(ag_model.parameters()).device\n",
    "\n",
    "num_voter = 11\n",
    "mask_pct = 0.3    \n",
    "    \n",
    "attack = TextFoolerJin2019\n",
    "\n",
    "if dataset_val == '100':\n",
    "    loaded_ag_100 = Dataset.load_from_disk('../data/filtered_ag_clean_100')\n",
    "    ag_100 = textattack.datasets.Dataset(convert_to_tuples(loaded_ag_100))\n",
    "    dataset = ag_100\n",
    "    dataset_name = 'ag-news100'\n",
    "elif dataset_val =='1000':\n",
    "    loaded_ag_1000 = Dataset.load_from_disk('../data/filtered_ag_clean_1000')\n",
    "    ag_1000 = textattack.datasets.Dataset(convert_to_tuples(loaded_ag_1000))\n",
    "    dataset = ag_1000\n",
    "    dataset_name = 'ag-news1000'\n",
    "else:\n",
    "    raise ValueError('Number of samples not supported')\n",
    "    \n",
    "if defense == \"default\":\n",
    "    ag_wrapper = textattack.models.wrappers.HuggingFaceModelWrapper(ag_model, ag_tokenizer)\n",
    "    print(ag_wrapper)\n",
    "elif defense == \"logit\":\n",
    "    ag_wrapper = MaskDemaskWrapper(ag_model, ag_tokenizer, ag_fill_mask, num_voter, mask_pct, 'logit')\n",
    "elif defense == 'maj_log':\n",
    "    ag_wrapper = MaskDemaskWrapper(ag_model, ag_tokenizer, ag_fill_mask, num_voter, mask_pct, 'maj_log')\n",
    "elif defense == \"one_hot\":\n",
    "    ag_wrapper = MaskDemaskWrapper(ag_model, ag_tokenizer, ag_fill_mask, num_voter, mask_pct, 'maj_one_hot')\n",
    "else:\n",
    "    raise ValueError('Not a valid defense type.')\n",
    "    \n",
    "print(f'using num_voter = {num_voter} and mask_pct = {mask_pct} with dataset = {dataset_name}...')\n",
    "\n",
    "# Parse the attack name\n",
    "attack_name = parse_attack_name(attack)\n",
    "attack = attack.build(ag_wrapper)\n",
    "\n",
    "\n",
    "# change candidate size\n",
    "attack.transformation.max_candidates = cand_size\n",
    "# adjust attack threshold to match Li et al. 2023 (0.5 theshold for AgNews Universal sentences encoder):\n",
    "attack.constraints[2] = UniversalSentenceEncoder(metric = 'angular', threshold = 0.5, \n",
    "                                                 window_size = 15, skip_text_shorter_than_window=True, \n",
    "                                                 compare_against_original=False)\n",
    "\n",
    "# Set up arguments for the attack\n",
    "attack_args = textattack.AttackArgs(\n",
    "    num_examples=len(dataset),\n",
    "    log_to_csv=f'{attack_name}_{dataset_name}_candsize{cand_size}_mp{mask_pct}_nv{num_voter}_{defense}_log.csv',\n",
    "    checkpoint_interval=25, \n",
    "    checkpoint_dir=\"chkpts_2\", \n",
    "    disable_stdout=True\n",
    ")\n",
    "# Perform the attack and save the results\n",
    "attacker = textattack.Attacker(attack, dataset, attack_args)\n",
    "attacker.attack_dataset()\n",
    "\n",
    "print(f'The above are results for {attack_name}_{dataset_name}_candsize{cand_size}_mp{mask_pct}_nv{num_voter}_{defense}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb818bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
