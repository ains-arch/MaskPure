{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdfeba2e",
   "metadata": {},
   "source": [
    "# Ensuring certifiable robustness on a dataset\n",
    "### First, imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0838e12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-15 18:10:55.368655: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-15 18:10:56.993566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import time\n",
    "import sys\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "import textattack\n",
    "from datasets import Dataset\n",
    "from scipy.special import comb\n",
    "from scipy.stats import beta as scipy_beta\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertForMaskedLM, pipeline\n",
    "\n",
    "sys.path.append('../')\n",
    "from eval_utils import *\n",
    "sys.path.pop()\n",
    "\n",
    "# set a seed, because reproducability is cool\n",
    "np.random.seed(int(hashlib.sha256('Harrison Gietz'.encode('utf-8')).hexdigest(), 16) % 2**32)\n",
    "torch.cuda.empty_cache()\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='transformers.pipelines')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff4a174",
   "metadata": {},
   "source": [
    "### Import models and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2178e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "ag_tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-ag-news\")\n",
    "ag_model = AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-ag-news\")\n",
    "ag_model.to(device)\n",
    "ag_pipeline = pipeline('sentiment-analysis', model=ag_model, tokenizer=ag_tokenizer)\n",
    "ag_pipeline.device = next(ag_model.parameters()).device\n",
    "\n",
    "ag_model_directory = \"../../../models/bert-uncased_maskedlm_agnews_july31\"\n",
    "finetuned_ag_maskedlm = BertForMaskedLM.from_pretrained(ag_model_directory)\n",
    "finetuned_ag_maskedlm.to(device)\n",
    "\n",
    "ag_raw_maskedlm = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "ag_raw_maskedlm.to(device)\n",
    "raw_fill_mask = pipeline('fill-mask', model=ag_raw_maskedlm, tokenizer=ag_tokenizer)\n",
    "raw_fill_mask.device = next(ag_model.parameters()).device\n",
    "\n",
    "ag_fill_mask = pipeline(\"fill-mask\", model=finetuned_ag_maskedlm, tokenizer=ag_tokenizer)\n",
    "ag_fill_mask.device = next(ag_model.parameters()).device\n",
    "\n",
    "loaded_ag_1000 = Dataset.load_from_disk('../data/filtered_ag_clean_1000')\n",
    "ag_1000 = textattack.datasets.Dataset(convert_to_tuples(loaded_ag_1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "499c3052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining placeholders\n",
    "def mask_and_demask(filtered_dataset_text, \n",
    "                    tokenizer,\n",
    "                    fill_mask,\n",
    "                    num_voter,\n",
    "                    mask_pct,\n",
    "                    pos_weights=None,\n",
    "                    output_text_token_len=False):\n",
    "    \"\"\"\n",
    "    Applies a process of masking and demasking on input data, repeating the process for a specified number of times for each sample.\n",
    "    \n",
    "    Parameters:\n",
    "    filtered_dataset_text (list): List of text strings from the filtered dataset.\n",
    "    tokenizer (object): The tokenizer to use.\n",
    "    num_voter (int, optional): Number of times the mask and demask process is repeated per sample. Defaults to 5.\n",
    "    verbose (bool, optional): Whether to print processing steps. Defaults to True.\n",
    "    fill_mask (transformers.pipeline, optional): The fill-mask pipeline for unmasking tokens. Defaults to pipeline(\"fill-mask\", model=\"bert-base-uncased\", tokenizer=\"bert-base-uncased\").\n",
    "    mask_pct (float, optional): The percentage of tokens to mask. Defaults to 0.2.\n",
    "    pos_weights (dict, optional): A dictionary mapping POS tags to weights. If specified, tokens with the corresponding POS tags will be more likely to be masked.\n",
    "\n",
    "    Returns:\n",
    "    A list of modified text strings.\n",
    "    \"\"\"\n",
    "    modified_adv_texts = []\n",
    "    v_convert_tokens_to_string = np.vectorize(tokenizer.convert_tokens_to_string, signature='(n)->()', otypes = [object])\n",
    "    for i, example in enumerate(filtered_dataset_text):\n",
    "        # Generate all masked versions in one operation, for each text\n",
    "        # unfortunately, this cannot be parallelized for multiple strings because nltk (used inside the function) runs on CPU only\n",
    "        # ...and is not compatible with np arrays\n",
    "        masked_texts, tokenized_masked_texts = mask_random_tokens(example, \n",
    "                                                                tokenizer, \n",
    "                                                                mask_pct=mask_pct, \n",
    "                                                                n=num_voter, \n",
    "                                                                pos_weights=pos_weights, \n",
    "                                                                return_separated=True)\n",
    "        text_token_len = tokenized_masked_texts.shape[1]\n",
    "        replace_idxs = np.argwhere(tokenized_masked_texts == '[MASK]')\n",
    "        # Unmask the texts and save the results\n",
    "        unmasked_text_suggestions = fill_mask([list(masked_text) for masked_text in masked_texts], top_k = 1)\n",
    "        replacement_tokens = [token_info[0]['token_str']  \n",
    "                              for sentence in unmasked_text_suggestions for token_info in sentence]\n",
    "        tokenized_masked_texts[replace_idxs[:, 0], replace_idxs[:, 1]] = replacement_tokens\n",
    "        unmasked = v_convert_tokens_to_string(tokenized_masked_texts).reshape(-1,)\n",
    "        [modified_adv_texts.append(unmasked[i]) for i in range(num_voter)]\n",
    "    \n",
    "    if not output_text_token_len:\n",
    "        return modified_adv_texts, None\n",
    "    else:\n",
    "        return modified_adv_texts, text_token_len\n",
    "\n",
    "\n",
    "def get_class_tally(inputs, pipeline, num_voter):\n",
    "    \"\"\"\n",
    "    Takes a list (of len=num_voter) modified inputs and returns a tally of predicted classes (either 2 or 4 classes depending on the pipeline)\n",
    "    \"\"\"       \n",
    "    results = pipeline(inputs,top_k=None)\n",
    "    final_results = []\n",
    "\n",
    "    if len(results[0]) != pipeline.model.config.num_labels:\n",
    "        raise ValueError(f'Pipeline number of labels ({pipeline.model.config.num_labels}) '\n",
    "                         f'and inner input text list length ({len(results)} outer and {len(results[0])} inner) '\n",
    "                         ' must have matching dims')\n",
    "    else: \n",
    "        num_labels = len(results[0])\n",
    "\n",
    "    if num_labels == 2:\n",
    "        vote_tally = {'LABEL_0': 0, 'LABEL_1': 0}\n",
    "    elif num_labels == 4:\n",
    "        vote_tally = {'LABEL_0': 0, 'LABEL_1': 0, 'LABEL_2': 0, 'LABEL_3': 0}\n",
    "    else: \n",
    "        raise ValueError(f'Unsupported number of labels ({num_labels}) in your pipeline. '\n",
    "                         'Requires 2 (imdb) or 4 (agnews)')\n",
    "\n",
    "    for dict_list in results:\n",
    "        top_score_val = 0\n",
    "        top_score_label = None\n",
    "        for dic in dict_list:\n",
    "            if dic['score'] > top_score_val:\n",
    "                top_score_val = dic['score']\n",
    "                top_score_label = dic['label']\n",
    "        vote_tally[top_score_label] += 1\n",
    "#             print('vote_tally', vote_tally)\n",
    "    top_label_overall = find_max_labels(vote_tally)\n",
    "\n",
    "#     # Calculate the total sum\n",
    "#     total = sum(vote_tally.values())\n",
    "#     # Create a new dictionary where the values are divided by the total\n",
    "#     normalized_vote_tally = {k: v/total for k, v in vote_tally.items()}\n",
    "    return vote_tally\n",
    "        \n",
    "def eq_6(label_count, num_certify, alpha):\n",
    "    return scipy_beta.ppf(alpha, label_count, num_certify - label_count + 1)\n",
    "\n",
    "def classifier_g(text, mask_pct, num_voter, output_token_len=False):\n",
    "    modified_adv_texts, text_token_len = mask_and_demask(filtered_dataset_text=[text], \n",
    "                                             tokenizer=ag_tokenizer, \n",
    "                                             fill_mask=ag_fill_mask,\n",
    "                                             num_voter=num_voter,\n",
    "                                             mask_pct=mask_pct,\n",
    "                                             output_text_token_len=output_token_len\n",
    "                                            )\n",
    "    counts = get_class_tally(modified_adv_texts, pipeline=ag_pipeline, num_voter=num_voter)\n",
    "    if not output_token_len:\n",
    "        return counts\n",
    "    else:\n",
    "        return counts, text_token_len\n",
    "\n",
    "def predict(text, mask_pct, num_voter):\n",
    "    counts = classifier_g(text, mask_pct, num_voter)\n",
    "    c_hat = find_max_labels(counts)\n",
    "    p_c_hat = counts[c_hat[0]] / num_voter\n",
    "    return c_hat, p_c_hat\n",
    "\n",
    "def certify(text, label, mask_pct, num_voter, num_certify, alpha):\n",
    "    \"\"\"\n",
    "    takes a text and some other params and does some certification stuff\n",
    "    returns: (d, rate)\n",
    "        d = number of adversaries that our proofs show this text is robust to, when using maskpure\n",
    "        rate = approx. percent equivalent of d/len(text), where len(text) is measured in tokens, not characters\n",
    "            Basically, \"what percent of this text can we mask such that it is still classified correctly?\"\n",
    "    \"\"\"\n",
    "    top_label, p_c = predict(text, mask_pct=mask_pct, num_voter=num_voter)\n",
    "    if label not in top_label:\n",
    "        return None, -float('inf') #the text is not robust to any adversaries since the top guess was not the correct one\n",
    "    else:\n",
    "        counts, jx = classifier_g(text, mask_pct, num_certify, output_token_len=True)\n",
    "#         print('counts: ', counts)\n",
    "#         print('text token length, jx: ', jx)\n",
    "        py = eq_6(counts[label], num_certify, alpha)  # Using Eq. (6) from Zeng et al. to find a lower bound for the prob of success\n",
    "#         print('py: ', py)\n",
    "        beta = counts[label] / num_certify\n",
    "#         print('beta: ', beta)\n",
    "        d = 0\n",
    "        kx = round(jx*(1 - mask_pct))\n",
    "#         print('rounded kx val: ', kx)\n",
    "        while d < jx:\n",
    "            delta = 1 - (comb(jx - d, kx) / comb(jx, kx))\n",
    "#             print('delta: ', delta)\n",
    "#             print('the thing that should be greater than 0.5: ', py - beta * delta)\n",
    "            if py - beta * delta > 0.5:\n",
    "                d += 1\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        return d, d/jx*100\n",
    "    \n",
    "def custom_sort(item):\n",
    "    if item is None:\n",
    "        return -float('inf')\n",
    "    return item\n",
    "\n",
    "def median(lst):\n",
    "    n = len(lst)\n",
    "    return lst[n // 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018b7423",
   "metadata": {},
   "source": [
    "### Results for 80% masking rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0247407d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iterations done. Time elapsed: 1.27 minutes. Estimated time remaining: 1269.98 minutes.\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05 #confidence of certificate\n",
    "num_certify = 1000\n",
    "num_voter = 25\n",
    "mask_pct = 0.8\n",
    "\n",
    "# run certify procedure in loop\n",
    "# run certify procedure in loop\n",
    "certificate_list = []  # used to record 7 size of perturbations each sample text can handle before misclassification\n",
    "certified_rates = []\n",
    "start_time = time.time()  # Start timing\n",
    "\n",
    "for i in range(len(loaded_ag_1000)):\n",
    "    label = f'LABEL_{loaded_ag_1000[\"label\"][i]}'\n",
    "    text = loaded_ag_1000['text'][i]\n",
    "    d, rate = certify(text, label, mask_pct, num_voter, num_certify, alpha)\n",
    "    certificate_list.append(d)\n",
    "    certified_rates.append(rate)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        elapsed_time_minutes = (time.time() - start_time) / 60\n",
    "        estimated_time_remaining_minutes = (elapsed_time_minutes / (i + 1)) * (len(loaded_ag_1000) - i - 1)\n",
    "        print(f'{i} iterations done. Time elapsed: {elapsed_time_minutes:.2f} minutes. '\n",
    "              f'Estimated time remaining: {estimated_time_remaining_minutes:.2f} minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81b69ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Median: 2\n",
      "\n",
      "Median: 5.0\n"
     ]
    }
   ],
   "source": [
    "sorted_certificates = sorted(certificate_list, key=custom_sort)\n",
    "# print(\"Sorted Absolute Certificates:\", sorted_certificates)\n",
    "print()\n",
    "print(\"Median:\", median(sorted_certificates))\n",
    "\n",
    "sorted_rates = sorted(certified_rates)\n",
    "# print(\"Sorted Rates:\", sorted_rates)\n",
    "print()\n",
    "print(\"Median:\", median(sorted_rates))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a026dc5",
   "metadata": {},
   "source": [
    "### If we count the number of \"None\" entries, we can get an idea of the raw accuracy as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dec3360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.0\n"
     ]
    }
   ],
   "source": [
    "def calculate_none_percentage(lst):\n",
    "    none_count = lst.count(None)\n",
    "    total_entries = len(lst)\n",
    "    percentage = (none_count / total_entries) * 100\n",
    "    return round(percentage, 2)\n",
    "\n",
    "print(100 - calculate_none_percentage(sorted_certificates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a4af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
